{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Student Attendance System with CNN & OpenCV\n",
    "============================================\n",
    "\n",
    "A lightweight attendance system using:\n",
    "- **CNN** for face recognition (M_2B_CNN.ipynb style)\n",
    "- **OpenCV Haar Cascade** for multi-face detection\n",
    "- **85% confidence threshold** to prevent false positives\n",
    "\n",
    "This script has cell-style comments for easy conversion to Jupyter notebook.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a6c4b",
   "metadata": {},
   "source": [
    "# Cell 1: Imports & Configuration\n",
    "Import necessary libraries and set up configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a28f4a",
   "metadata": {
    "title": "Cell 1: Imports & Configuration"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration - works in both .py and .ipynb\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).parent.absolute()\n",
    "except NameError:\n",
    "    # For Jupyter notebooks - go up from notebooks/ folder\n",
    "    BASE_DIR = Path('..').absolute() if Path('../dataset').exists() else Path('.').absolute()\n",
    "DATASET_DIR = BASE_DIR / 'dataset'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "ATTENDANCE_DIR = BASE_DIR / 'attendance_logs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "ATTENDANCE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Hyperparameters - OPTIMIZED FOR MEMORY EFFICIENCY\n",
    "IMG_SIZE = 64  # Reduced from 160/224 to save memory\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16  # Smaller batch size for stability\n",
    "EPOCHS = 30\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MIN_CONFIDENCE = 0.50  # 50% threshold - adjusted for real-world camera conditions\n",
    "MIN_FACE_SIZE = 30  # Minimum face size for detection\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Student Attendance System - Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "print(f\"Dataset Directory: {DATASET_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Confidence Threshold: {MIN_CONFIDENCE * 100}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978ea60",
   "metadata": {},
   "source": [
    "# Cell 2: Data Preprocessing Functions\n",
    "Functions for loading and preprocessing face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ee7bd",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 2: Data Preprocessing Functions"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img, target_size=(IMG_SIZE, IMG_SIZE)):\n",
    "    \"\"\"\n",
    "    Preprocess a single image for CNN input.\n",
    "    \n",
    "    Args:\n",
    "        img: BGR image (numpy array)\n",
    "        target_size: Tuple of (height, width) for resizing\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    # Resize to target size\n",
    "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def check_image_quality(img):\n",
    "    \"\"\"\n",
    "    Check if an image meets quality standards.\n",
    "    \n",
    "    Args:\n",
    "        img: Image (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (is_valid, reason)\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        return False, \"Image is None\"\n",
    "    \n",
    "    # Check minimum dimensions\n",
    "    if img.shape[0] < 30 or img.shape[1] < 30:\n",
    "        return False, \"Image too small\"\n",
    "    \n",
    "    # Convert to grayscale for quality checks\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # Check blur using Laplacian variance\n",
    "    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    if blur_score < 20:\n",
    "        return False, f\"Too blurry (score: {blur_score:.1f})\"\n",
    "    \n",
    "    # Check contrast\n",
    "    contrast = np.std(gray)\n",
    "    if contrast < 10:\n",
    "        return False, f\"Low contrast (score: {contrast:.1f})\"\n",
    "    \n",
    "    return True, \"OK\"\n",
    "\n",
    "\n",
    "print(\"Data preprocessing functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd5b75",
   "metadata": {},
   "source": [
    "# Cell 3: Haar Cascade Multi-Face Detection\n",
    "Initialize OpenCV Haar Cascade for detecting multiple faces in a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4a695",
   "metadata": {
    "title": "Cell 3: Haar Cascade Multi-Face Detection"
   },
   "outputs": [],
   "source": [
    "# Load Haar Cascade classifiers\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "face_cascade_alt = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml'\n",
    ")\n",
    "\n",
    "\n",
    "def detect_multiple_faces(image, scale_factor=1.1, min_neighbors=5):\n",
    "    \"\"\"\n",
    "    Detect ALL faces in an image using Haar Cascade.\n",
    "    Supports detecting multiple faces simultaneously.\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image (numpy array)\n",
    "        scale_factor: Scale factor for cascade\n",
    "        min_neighbors: Minimum neighbors for detection\n",
    "    \n",
    "    Returns:\n",
    "        List of (x, y, w, h) tuples for each detected face\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply histogram equalization for better detection\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    \n",
    "    # Detect faces using primary cascade\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=scale_factor,\n",
    "        minNeighbors=min_neighbors,\n",
    "        minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    # If no faces found, try alternative cascade\n",
    "    if len(faces) == 0:\n",
    "        faces = face_cascade_alt.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=scale_factor,\n",
    "            minNeighbors=min_neighbors,\n",
    "            minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    \n",
    "    return list(faces) if len(faces) > 0 else []\n",
    "\n",
    "\n",
    "def extract_face(image, face_rect, margin=0.1):\n",
    "    \"\"\"\n",
    "    Extract and preprocess a face from an image.\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image\n",
    "        face_rect: (x, y, w, h) tuple\n",
    "        margin: Margin around face as fraction\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed face image or None\n",
    "    \"\"\"\n",
    "    x, y, w, h = face_rect\n",
    "    \n",
    "    # Add margin\n",
    "    x1 = max(0, int(x - w * margin))\n",
    "    y1 = max(0, int(y - h * margin))\n",
    "    x2 = min(image.shape[1], int(x + w + w * margin))\n",
    "    y2 = min(image.shape[0], int(y + h + h * margin))\n",
    "    \n",
    "    # Extract face region\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    \n",
    "    if face.size == 0:\n",
    "        return None\n",
    "    \n",
    "    return face\n",
    "\n",
    "\n",
    "print(\"Haar Cascade face detection initialized.\")\n",
    "print(f\"  - Primary cascade loaded: {not face_cascade.empty()}\")\n",
    "print(f\"  - Alt cascade loaded: {not face_cascade_alt.empty()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314ca6d",
   "metadata": {},
   "source": [
    "# Cell 4: Dataset Loading\n",
    "Load images from the dataset directory and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00823dfa",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 4: Dataset Loading"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, img_size=(IMG_SIZE, IMG_SIZE)):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from the dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset directory\n",
    "        img_size: Target image size\n",
    "    \n",
    "    Returns:\n",
    "        X (images), y (labels), class_names (list of student IDs)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Get all student directories (sorted for consistency)\n",
    "    student_dirs = sorted([d for d in dataset_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f\"\\nLoading dataset from: {dataset_path}\")\n",
    "    print(f\"Found {len(student_dirs)} students:\")\n",
    "    \n",
    "    for idx, student_dir in enumerate(student_dirs):\n",
    "        student_id = student_dir.name\n",
    "        class_names.append(student_id)\n",
    "        \n",
    "        # Get all image files\n",
    "        image_files = list(student_dir.glob('*.jpg')) + list(student_dir.glob('*.png'))\n",
    "        print(f\"  [{idx}] {student_id}: {len(image_files)} images\")\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            \n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Preprocess image\n",
    "            img = preprocess_image(img, img_size)\n",
    "            \n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(idx)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"  Total images: {len(X)}\")\n",
    "    print(f\"  Image shape: {X.shape[1:]}\")\n",
    "    print(f\"  Number of classes: {len(class_names)}\")\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Loading Dataset...\")\n",
    "print(\"=\" * 50)\n",
    "X, y, CLASS_NAMES = load_dataset(DATASET_DIR)\n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c930d",
   "metadata": {},
   "source": [
    "# Cell 5: Train/Test Split\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbba9f",
   "metadata": {
    "title": "Cell 5: Train/Test Split"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Splitting Dataset...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} images\")\n",
    "print(f\"Test set: {len(X_test)} images\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b475f6",
   "metadata": {},
   "source": [
    "# Cell 6: CNN Model Architecture\n",
    "Build a lightweight CNN model following M_2B_CNN.ipynb style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333126e5",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 6: CNN Model Architecture"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a lightweight CNN model for face recognition.\n",
    "    Architecture follows M_2B_CNN.ipynb style.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Tuple of (height, width, channels)\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        keras.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1: Conv -> Pool -> Dropout\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2: Conv -> Pool -> Dropout\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Building CNN Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model\n",
    "model = create_cnn_model((IMG_SIZE, IMG_SIZE, CHANNELS), NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eac666",
   "metadata": {},
   "source": [
    "# Cell 7: Data Augmentation & Model Training\n",
    "Set up data augmentation and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9b69c",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 7: Data Augmentation & Model Training"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data augmentation for better generalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(MODEL_DIR / 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(str(MODEL_DIR / 'final_model.keras'))\n",
    "print(f\"\\nModel saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192a272",
   "metadata": {},
   "source": [
    "# Cell 8: Training Visualization\n",
    "Plot training history (accuracy and loss curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b5436",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 8: Training Visualization"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Generating Training Plots...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(MODEL_DIR / 'training_history.png'), dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training plots saved to: {MODEL_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca97fa",
   "metadata": {},
   "source": [
    "# Cell 9: Model Evaluation\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c801a",
   "metadata": {
    "title": "Cell 9: Model Evaluation"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Evaluating Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred_conf = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# High confidence accuracy (with 85% threshold)\n",
    "high_conf_mask = y_pred_conf >= MIN_CONFIDENCE\n",
    "if high_conf_mask.sum() > 0:\n",
    "    high_conf_acc = np.mean(y_pred[high_conf_mask] == y_test[high_conf_mask])\n",
    "    print(f\"High Confidence (>={MIN_CONFIDENCE*100:.0f}%) Accuracy: {high_conf_acc * 100:.2f}%\")\n",
    "    print(f\"  Predictions above threshold: {high_conf_mask.sum()} / {len(y_test)}\")\n",
    "else:\n",
    "    print(\"No predictions above confidence threshold!\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=CLASS_NAMES))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(MODEL_DIR / 'confusion_matrix.png'), dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {MODEL_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416c4a3",
   "metadata": {},
   "source": [
    "# Cell 10: Multi-Face Recognition Function\n",
    "Core function for detecting and recognizing multiple faces in a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43b0ef",
   "metadata": {
    "title": "Cell 10: Multi-Face Recognition Function"
   },
   "outputs": [],
   "source": [
    "def recognize_faces(image, model, class_names, confidence_threshold=MIN_CONFIDENCE):\n",
    "    \"\"\"\n",
    "    Detect and recognize ALL faces in an image.\n",
    "    Uses Haar Cascade for detection and CNN for recognition.\n",
    "    \n",
    "    Args:\n",
    "        image: BGR image from camera or file\n",
    "        model: Trained CNN model\n",
    "        class_names: List of student IDs\n",
    "        confidence_threshold: Minimum confidence for valid prediction\n",
    "    \n",
    "    Returns:\n",
    "        List of (face_rect, student_id, confidence) tuples\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Detect all faces using Haar Cascade\n",
    "    faces = detect_multiple_faces(image)\n",
    "    \n",
    "    for face_rect in faces:\n",
    "        # Extract face region\n",
    "        face = extract_face(image, face_rect)\n",
    "        \n",
    "        if face is None:\n",
    "            continue\n",
    "        \n",
    "        # Check face quality\n",
    "        is_valid, reason = check_image_quality(face)\n",
    "        if not is_valid:\n",
    "            continue\n",
    "        \n",
    "        # Preprocess for CNN\n",
    "        face_preprocessed = preprocess_image(face, (IMG_SIZE, IMG_SIZE))\n",
    "        face_input = np.expand_dims(face_preprocessed, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(face_input, verbose=0)[0]\n",
    "        pred_class = np.argmax(predictions)\n",
    "        confidence = predictions[pred_class]\n",
    "        \n",
    "        # Only accept high confidence predictions (no false positives)\n",
    "        if confidence >= confidence_threshold:\n",
    "            student_id = class_names[pred_class]\n",
    "        else:\n",
    "            student_id = \"Unknown\"\n",
    "        \n",
    "        results.append((face_rect, student_id, confidence))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"\\nMulti-face recognition function ready!\")\n",
    "print(f\"  - Confidence threshold: {MIN_CONFIDENCE * 100}%\")\n",
    "print(f\"  - Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85e0de",
   "metadata": {},
   "source": [
    "# Cell 11: Attendance Tracking Class\n",
    "Class for tracking student attendance and preventing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82056f08",
   "metadata": {
    "title": "Cell 11: Attendance Tracking Class"
   },
   "outputs": [],
   "source": [
    "class AttendanceTracker:\n",
    "    \"\"\"\n",
    "    Track student attendance with duplicate prevention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_names, cooldown_seconds=60):\n",
    "        self.class_names = class_names\n",
    "        self.cooldown = cooldown_seconds\n",
    "        self.attendance = {}  # {student_id: (first_seen, last_seen, count)}\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def mark_present(self, student_id, confidence):\n",
    "        \"\"\"\n",
    "        Mark a student as present.\n",
    "        \n",
    "        Args:\n",
    "            student_id: Student identifier\n",
    "            confidence: Prediction confidence\n",
    "        \n",
    "        Returns:\n",
    "            True if newly marked, False if already marked\n",
    "        \"\"\"\n",
    "        if student_id == \"Unknown\":\n",
    "            return False\n",
    "        \n",
    "        now = datetime.now()\n",
    "        \n",
    "        if student_id in self.attendance:\n",
    "            first_seen, last_seen, count = self.attendance[student_id]\n",
    "            # Check cooldown\n",
    "            if (now - last_seen).seconds < self.cooldown:\n",
    "                self.attendance[student_id] = (first_seen, now, count)\n",
    "                return False\n",
    "            self.attendance[student_id] = (first_seen, now, count + 1)\n",
    "        else:\n",
    "            self.attendance[student_id] = (now, now, 1)\n",
    "            print(f\"âœ“ {student_id} marked PRESENT (confidence: {confidence:.1%})\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get attendance summary string.\"\"\"\n",
    "        present = len(self.attendance)\n",
    "        total = len(self.class_names)\n",
    "        return f\"Present: {present}/{total}\"\n",
    "    \n",
    "    def save_to_csv(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Save attendance to CSV file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Optional custom path\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved file\n",
    "        \"\"\"\n",
    "        if filepath is None:\n",
    "            timestamp = self.session_start.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = ATTENDANCE_DIR / f'attendance_{timestamp}.csv'\n",
    "        \n",
    "        with open(filepath, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Student_ID', 'First_Seen', 'Last_Seen', 'Detections', 'Status'])\n",
    "            \n",
    "            for student_id in self.class_names:\n",
    "                if student_id in self.attendance:\n",
    "                    first, last, count = self.attendance[student_id]\n",
    "                    writer.writerow([\n",
    "                        student_id,\n",
    "                        first.strftime('%H:%M:%S'),\n",
    "                        last.strftime('%H:%M:%S'),\n",
    "                        count,\n",
    "                        'Present'\n",
    "                    ])\n",
    "                else:\n",
    "                    writer.writerow([student_id, '', '', 0, 'Absent'])\n",
    "        \n",
    "        print(f\"\\nAttendance saved to: {filepath}\")\n",
    "        return str(filepath)\n",
    "\n",
    "\n",
    "print(\"Attendance tracking system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de306e",
   "metadata": {},
   "source": [
    "# Cell 12: Real-Time Attendance System (Optional)\n",
    "Run the camera-based attendance system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403aef07",
   "metadata": {
    "title": "Cell 12: Real-Time Attendance System"
   },
   "outputs": [],
   "source": [
    "def run_realtime_attendance(model, class_names, camera_id=0, duration_seconds=None):\n",
    "    \"\"\"\n",
    "    Run real-time attendance system using camera.\n",
    "    \n",
    "    Controls:\n",
    "        'q' - Quit and save attendance\n",
    "        's' - Save attendance immediately\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNN model\n",
    "        class_names: List of student IDs\n",
    "        camera_id: Camera device ID\n",
    "        duration_seconds: Auto-stop after N seconds (None for manual)\n",
    "    \n",
    "    Returns:\n",
    "        AttendanceTracker object with results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"REAL-TIME ATTENDANCE SYSTEM\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Controls: 'q' = quit | 's' = save\")\n",
    "    print(f\"Confidence threshold: {MIN_CONFIDENCE * 100}%\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ERROR: Could not open camera!\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = AttendanceTracker(class_names)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Recognize faces\n",
    "        results = recognize_faces(frame, model, class_names)\n",
    "        \n",
    "        # Draw results\n",
    "        for face_rect, student_id, confidence in results:\n",
    "            x, y, w, h = face_rect\n",
    "            tracker.mark_present(student_id, confidence)\n",
    "            \n",
    "            # Simple color: green for recognized, red for unknown\n",
    "            color = (0, 255, 0) if student_id != \"Unknown\" else (0, 0, 255)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            # Label\n",
    "            label = f\"{student_id}: {confidence:.0%}\"\n",
    "            cv2.putText(frame, label, (x, y - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        # Simple status\n",
    "        cv2.putText(frame, tracker.get_summary(), (10, 25),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Attendance', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            tracker.save_to_csv()\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            tracker.save_to_csv()\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nFinal: {tracker.get_summary()}\")\n",
    "    return tracker\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTo test with camera, uncomment and run:\")\n",
    "print(\"  tracker = run_realtime_attendance(model, CLASS_NAMES)\")\n",
    "print(\"\\nTo test with an image:\")\n",
    "print(\"  results = recognize_faces(cv2.imread('path/to/image.jpg'), model, CLASS_NAMES)\")\n",
    "print(\"  for face_rect, name, conf in results:\")\n",
    "print(\"      print(f'{name}: {conf:.1%}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a600a7",
   "metadata": {
    "title": "Main - Uncomment to run real-time attendance"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tracker = run_realtime_attendance(model, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53671b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
